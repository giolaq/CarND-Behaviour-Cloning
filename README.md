#**Behavioral Cloning**
---

**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image1]: ./pics/nvidia_model.png "NVIDIA MODEL"
[image2]: ./pics/loss.png "Loss visualization"
[leftcamera]: ./pics/left.jpg "Left camera"
[centercamera]: ./pics/center.jpg "Center camera"
[rightcamera]: ./pics/right.jpg "Right camera"
[hist]: ./pics/hist.jpeg "Dataset not augmented"
[hist2]: ./pics/hist2.jpeg "Dataset augmented"

## Project files description
####1.
My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network
* writeup_report.md (this file) summarizing the results

####2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing
```sh
python drive.py model.h5
```

####3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

###Model Architecture and Training Strategy

####1. An appropriate model architecture has been employed

The NVIDIA model was studyed and used as training model.

![NVIDIA Model][image1]

In the first layer according to [NVIDIA paper](https://arxiv.org/pdf/1604.07316v1.pdf) the image was normalized.

The model continues with three layers with 2x2 strides and a 5x5 kernel, and two convolutional layers with non-strided convolution with 3x3 kernel size.

Following there are 3 fully connected layers giving as output
the steering angle that will drive our car.

The loss was visualized:
![image2]
####2. Attempts to reduce overfitting in the model

The model contains dropout layers in order to reduce overfitting, the dropout parameter is at line 169 of model.py.

The model was trained and validated on different data sets to ensure that the model was not overfitting (code line 264-265).
Different dataset and batches was generated by generator function to improve performance.

### Data preprocessing and augmentation
To train the model I've used data recorded from the simulator.
The Udacity data was used for validation.

The dataset is composed by sets of 3 images taken from 3 different car cameras:
left, center, right

![][leftcamera]
![][centercamera]
![][rightcamera]

Associated to images there is the steering angle for the time the picture
are recorded.

Analyzing the Udacity data and the generated ones driving in the first track, we can see there are many "turn left" pictures by the track path.

![][hist]


To augment the data I used these steps:


1. **Choose random camera image from center, left and right:** The simulator has three camera views namely; center, left and right views. Using the left and right images, we add and subtract 0.25 to the steering angles respectively to make up for the camera offsets.
2. **Translate image and compensate steering angles:** Since the original image size is 160x320 pixels, we randomly translate image to the left or right and compensate for the translation in the steering angles with 0.008 per pixel of translation. We then crop a region of interest of 120x220 pixel from the image.
3. **Randomly flip image:** In other to balance left and right images, we randomly flip images and change sign on the steering angles.
4. **Brightness Augmentation** We simulate different brightness occasions by converting image to HSV channel and randomly scaling the V channel.

After augmentation we have a more well distributed dataset
![][hist2]
